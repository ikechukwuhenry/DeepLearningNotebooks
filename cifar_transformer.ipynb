{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ikechukwuhenry/DeepLearningNotebooks/blob/main/cifar_transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xwPp9RGbp8WD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2055704f-998d-4dd4-d4d6-586aac059d05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow_addons\n",
            "  Downloading tensorflow_addons-0.16.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[?25l\r\u001b[K     |▎                               | 10 kB 22.9 MB/s eta 0:00:01\r\u001b[K     |▋                               | 20 kB 11.4 MB/s eta 0:00:01\r\u001b[K     |▉                               | 30 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |█▏                              | 40 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 51 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |█▊                              | 61 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██                              | 71 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 81 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 92 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |███                             | 102 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███▏                            | 112 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 122 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 133 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████                            | 143 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 153 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 163 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████                           | 174 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 184 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 194 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 204 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 215 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 225 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 235 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████                         | 245 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 256 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 266 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 276 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 286 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 296 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 307 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 317 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 327 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 337 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 348 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 358 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 368 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 378 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 389 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 399 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 409 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 419 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 430 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 440 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 450 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 460 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 471 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 481 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 491 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 501 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 512 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 522 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 532 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 542 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 552 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 563 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 573 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 583 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 593 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 604 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 614 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 624 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 634 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 645 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 655 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 665 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 675 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 686 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 696 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 706 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 716 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 727 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 737 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 747 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 757 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 768 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 778 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 788 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 798 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 808 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 819 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 829 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 839 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 849 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 860 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 870 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 880 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 890 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 901 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 911 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 921 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 931 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 942 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 952 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 962 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 972 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 983 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 993 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.0 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.0 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.0 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 1.0 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.0 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.1 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 1.1 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 1.1 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.1 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.1 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.1 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.1 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.1 MB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow_addons) (2.7.1)\n",
            "Installing collected packages: tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.16.1\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "!pip install tensorflow_addons\n",
        "import tensorflow_addons as tfa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T2pjg3RQqA7q",
        "outputId": "ef8ff856-4de7-41f9-e974-e49f374c5cf4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 2s 0us/step\n",
            "170508288/170498071 [==============================] - 2s 0us/step\n",
            "x_train: (50000, 32, 32, 3) - y_train: (50000, 1)\n",
            "x_test: (10000, 32, 32, 3) - y_test: (10000, 1)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from keras.datasets import cifar10\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "print(f\"x_train: {x_train.shape} - y_train: {y_train.shape}\")\n",
        "print(f\"x_test: {x_test.shape} - y_test: {y_test.shape}\")\n",
        "input_shape = x_train.shape[1:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "WWhRv2f9qJSb"
      },
      "outputs": [],
      "source": [
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.Normalization(),\n",
        "        layers.Resizing(72, 72),\n",
        "        layers.RandomFlip(\"horizontal\"),\n",
        "        # layers.RandomFlip('horizontal',input_shape=(input_shape[0], input_shape[1],3)),\n",
        "        layers.RandomRotation(factor=0.02),\n",
        "        layers.RandomZoom(\n",
        "            height_factor=0.2, width_factor=0.2\n",
        "        ),\n",
        "    ],\n",
        "    name=\"data_augmentation\",\n",
        ")\n",
        "data_augmentation.layers[0].adapt(x_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "XgFtc5rzqP_F"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.001\n",
        "weight_decay = 0.0001\n",
        "batch_size = 256\n",
        "num_epochs = 100\n",
        "image_size = 72\n",
        "patch_size = 6  \n",
        "num_patches = (image_size // patch_size) ** 2\n",
        "projection_dim = 64\n",
        "num_heads = 4\n",
        "transformer_units = [projection_dim * 2,projection_dim,] \n",
        "transformer_layers = 8\n",
        "mlp_head_units = [2048, 1024]\n",
        "num_classes = 10\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "t7CGZAcCqUFu",
        "outputId": "70e4a3e6-b8b9-4b50-a395-04c48401a2b6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-0.5, 31.5, 31.5, -0.5)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 288x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAADnCAYAAADl9EEgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAATXElEQVR4nO2d2Y9l51XF95nuuXPde6u6hu52D+40lu22MXQsnMGEOZEiMTwhIch/AvwrPIGUB3gJREFCQo6QQyx3Gtq4R/dQVe4abg13vufcM/HA67e2ZIScTbJ+j2fru3WmVUfa69t7e1VVCSHEHv7P+wQIIW4oTkKMQnESYhSKkxCjUJyEGCXUgvc/vQdTuZ89eQrXbVzYdB7/8CcfwjXf+4vvwdigP4CxoiphrPLcx31wXETE05LXnrJQtBghGN93v5H8chJiFIqTEKNQnIQYheIkxCgUJyFGoTgJMYpqpWjK/Zcf/RDGZvOZ87i2x37+R38MY+sbGzAWKmeJTBZts7+n+Cw0S8iXCb+chBiF4iTEKBQnIUahOAkxCsVJiFEoTkKMolopp8dDGNt/9gTG5rOp83h//QJc84//8Pcw9o33vwVjO1evwFhvw/33okCxXxS7pygKGPOVf3Oeh4OeWulCfpnhl5MQo1CchBiF4iTEKBQnIUahOAkxipqtPdjfh7HLm3gzerXRcx7//PAYrvnJB/8KY08fPoCxWqcDYzffest5/JvfxNnfVy6/gv9WXIOxPM9gzFMaE4Wh+gjILzH8chJiFIqTEKNQnIQYheIkxCgUJyFGoTgJMYqax2816zDWrkcwNh3PncezZAnXJHiqgozx/ntZnRzB2OPP3JvzP/7pR3DNm2+8DWPv3P41GHv7V/G6ZrMJY6ifETfEE345CTEKxUmIUShOQoxCcRJiFIqTEKNQnIQYRbVS0jTFscRtl4iIJKl7HIMnuHJjcu7uOyQi0omxrVBrtGBs5bntnuOXh3DN6f5LGPvkzk9h7Nffew/Gvvat34Kxm6+95jyuGinaXAtl0rduz7hj2qBvDV/5W/8bk0gboaEOI9d+VDlH7TfL/2P7KwDH+eUkxCgUJyFGoTgJMQrFSYhRKE5CjEJxEmIU1Uq5c/c/YCxXcs3D6cR5/Gh0DtfUlf8T2cptzYiIRCgPLSJ+6G40FscNuKZfw+dRLLF99M8/+AGMPXi+C2O//Qffdh6/eeMGXNPt47EWpY9vSKCMoYAugOIO+MrsinC1gLEixffx/MRdgnR4hO2vRYItv263C2NbOzswtjZYhzEJQEWWMpMjiLDUOqDJG7+chBiF4iTEKBQnIUahOAkxCsVJiFEoTkKMolopuy9x86z5GbY3loE7fV21Y7hmOjqFsXSFq1maTS3X7z4c+bhyI/JwLIxxU7N+hOeoPHv0GMZePH3hPH7j6lW45r3f/Q6MXX3zHRjrtHDDtny1ch4fj0ZwzewM2xvjZ5/A2Mn+cxhbzt3vVQrOT0RknuD3Q0r8/RmAyeciItd+xV0tJCJy89Yt5/G1rS24Jqrhd7/TceuFX05CjEJxEmIUipMQo1CchBiF4iTEKJ7Wm+Wv//KvYHB4jKdUv/Ob7zuP15oDuOaf/u5vYGzyFI9PuLa1DWON7qbzuBfizFns5TCWKpm/0RKvSzKcAW6BUQ1tZYp2u4s37l++dhnGttb7MFau3KMyZuMzuCZd4Iy95AkMzca4ACIAm8fbXXzuheAser7Cz8z3sVlR1fBvRj13QcXbX/sGXHPj9TdhbNAdOC0HfjkJMQrFSYhRKE5CjEJxEmIUipMQo1CchBhFtVLu3r0Dg4sUp8ofD90b5s8neLL1aM89hVpE5OMf/i2MrYe4Z87mhWvuQIgnTXsV3kS9SHEsV2wW38PneAHYG2vKJvUyw6Mr4hA/z1YD2zP10F1AEIoy3kEKGMtLfK9mE3ePKRFsb8TNNbgmqOOY7+H76Cl9sJY53mi/d+4uBljVcb+iP/zTP4Ox21/9Oq0UQv4/QXESYhSKkxCjUJyEGIXiJMQoFCchRlF7CF29gnuszDJsi9w/vu88PhrvwTUd98Z8ERG5fus6jJ1+6v5bIiI5GOMQRbiqI8uxPdCK8e3qd92VCiIinjK2oNdzp/pbTWy/VAWe5h0rvYwadVyNEwXu++8pM54L5R1Iprj/1Erw+IQwcF93nuLxDpXgaw5CbAUFPr62WOkzNWi5K1Y+evQQrvn3f/sxjN3+6tedx/nlJMQoFCchRqE4CTEKxUmIUShOQoxCcRJiFNVKqUpceeJVOBZUbgsjqJRmURm2UnrbGzA228O2wvnE3UiqE3TgmloNp+Uvb+Npx4NOG8aqAtszbZCWjxv4fni4YEL8DFeDRHB8tUgcuW0WH01xFpGVj5uaRYIrf9IlrqrJwPkHMf69RBnVUOb4fjQifD8aim3WAzZXrIzyePTgAYwh+OUkxCgUJyFGoTgJMQrFSYhRKE5CjEJxEmIU1UoZnuCUd7uL7YjRvruC4OXDMT4RZfLvZIKrGIrGDoxlifv8Ox7+n7Tdx7bNRh83cGrEivVR4Vi95n4E9RBbOoVS5aL0rBIvwNeNTtEHs0tERIIIvz5FrkwcB83EREQKMKXaV67MA/dQRKQosN0zWeCqmvkIv6s1YCGVCZ4CfniGm5Ah+OUkxCgUJyFGoTgJMQrFSYhRKE5CjEJxEmIU1UrxPGxv1Gq4SVaRunfnz8a4SZMX4cqChVJ1sLaGG2s1Y/f59+q4gqSjzBOJfFxdEiiDN8oSW0Fe5b5X2nyVXKkuKUGDrP85D3z+xQrFcKVFpoyWzzPF/lIqZ0Jg9/iKDRQo83ICxYKZLdzVUyIi+Qg3KGuG7ns1n2FrZly5K6Q0+OUkxCgUJyFGoTgJMQrFSYhRKE5CjKJmazc38SbwosIZ1P6Gu6/Pla/gHjyLFGe61ip8mhd6OPNajMAE5TOcZawKnFH2CzwluUhwdnIyOYWxEvQeisIBXlMpE6WVicyibAL3QVZ2keC/tZjjwohlOoex1RLffz8EPaFw0ljyDF9XpmT60znO1nrKfVxU7m/aaIava1oOYQzBLychRqE4CTEKxUmIUShOQoxCcRJiFIqTEKOoVkqoRLVN1M0198LuBt6gLDOcDs+VnjlxB+fYE9CPZpbgTchFiMcPlDn+X1YITqOPRoqVAsZaxDG+V9lKsQ5SbAEgu0RExBP380xm2C45O8PXlSjnkRb4PtY7oKeSr/0eji0THMtW+Jk1lN5JU/AejJf4uSx8bBUi+OUkxCgUJyFGoTgJMQrFSYhRKE5CjEJxEmIU1UoRJX2t2SybO33n8aWHW9IPKtyvSHFSpFnHvYyqvrvi48khbpufLHGlwhkochERWSS4ff9wiC2HDPTaqUf4ulYJtgDSFMfKAlspBbDGihzbA/ME/97hCb6P+yf4/rfX3ZbDziVlpEWE+z6tMvzyBMq3KajhCqSTM/czO8eFSRIok8oR/HISYhSKkxCjUJyEGIXiJMQoFCchRqE4CTGKaqUkKbYHJMS69kN3+jqs43SyZokoDoD4ypRqv3Snw1vtJlwzV/yS3SPcpOnoaB/GghJXJIT+ZefxVg3f+8UMn+NKqbRIc3wjVyCU5tiKmCjjByYprlpq7FyBsXnpPpHdIb73F/t4JEeS4vejVBqUZRE+/4Ox+xynGa4kaiiWDoJfTkKMQnESYhSKkxCjUJyEGIXiJMQoFCchRlGtlJ/d/wDGvAgvPZudOI+nKzyHJKtwqrlQphMHPj4PD9gAnYu4OqZSLIBUsQ5mqTJtGg9ylqdH7gqNFye4CVmi2CWRMvW6AjaFiEiKJkrXcLVQr9uBsRuXXoGx/s4OjE1RVVCOSz6iUml4dojfudVUeYcTfB+nhfsdyX3lHVCqexD8chJiFIqTEKNQnIQYheIkxCgUJyFGUbO1R7M9GFO38QLJ15XN7V6AfzFXJjlLhTNkQd29Ebl+DWcLK8GblzdTnLlsNvAk6o/v3oGxowN3Zvv2+2/DNb1tPCFc64vTABlZERFZc/fhaa/hyeFbdTCFWkQaSkyUrGYBUtuVMv6jVCZ2r73AvYwOHuICiJOxMn27dE/t9k5wQYLWiwn+nS+8ghDypUBxEmIUipMQo1CchBiF4iTEKBQnIUZRrZTZEu/Yns9wiroE8xMaDWyltJtKS/0Eb16eznCsAhOUowpPr84KZcOzch7KsGYJW/i6A2AdvHb7Lbhm4/WLMFYpllTsYZsoBX2fSsU0CzNsb+SKdVAqG/A9D72S+JkFiv3VUvr6LB/swth4jp/1fO7ehO8rcsorpREWgF9OQoxCcRJiFIqTEKNQnIQYheIkxCgUJyFGUa2U3ed4xEC2wlOvUa+aWOlHEykWwHyORxPMl8r4AdACP1KmFg/auLokA3aDiMhwhHv+VLFy3aG7D8+Tz/G9X3WxxdVbw5UW0wQ/syByWxWZMla8UuyBolLGPCt9jtqtrvP4cqmcu9JHSpkaIidL/F7tH7qrhURE0sD9zAJfmc5eYGsGwS8nIUahOAkxCsVJiFEoTkKMQnESYhSKkxCjqFaKFHhHf6lkyhdgbMFkhRsgtTu4cmOZ4T82nbmbLYmIBLG70sWrYQtgVmJrZqVYKZMVtjfSEt/HXNznOJ3j3xud42ueznG1ULbE97FYuGPa2I2l4MoTTxmf0Goqdk/DfW2LBX4u2QpXx9Q8bJvNFCsoUYpIGm1307O0xPdqPMHvPoJfTkKMQnESYhSKkxCjUJyEGIXiJMQoFCchRlGtlD/59p/D2Ms9XDXxn3fcs0FOjo7hmu3WZRgrPZzXfjZ6DGO+57Y+MmXuxjzBNkUU47khRYAblGUFPv/Qc1eDTM5wFUPcwNbM6cw9KVtEpBHhc6ylwLqJ8f/vWYDvo5dim6XTwPcjCN2WSZJiK6VSpqJnyiydcY7tqmYHTz+Pm+734GyK7RIv46wUQn5hoDgJMQrFSYhRKE5CjEJxEmIUNVv77q3fg7H0K+7N7SIi777xO87jy4myKTvFPWLSBP+t4+t4+vb9T+86jw/PD+GaC9ubMPb67Xdh7PvJj2BsPv0MxjKQHV6c4uxkWO/BWGuEM7L9gbs/j4hIA2zcT8HoARGRueBs51mGzz/awudRKCNA4O+F+DWux8r0bSUTPV7inlB55X5mJRitISJSKlljBL+chBiF4iTEKBQnIUahOAkxCsVJiFEoTkKMolopT/Y+hbFWA/dmafbc/YB663gic5Yr4x08HNu8hDco72xcch4/ePo5XHP1+haMNa5dgLFB/2cwdvEStiNmI7et88oGHgvx3Xd/H8ZaNdyfJx7gPk3jyZnzuNZ3aHf4EsYejp/DWFVgy2F57h6R0Anw+1Yr8Tdmu7cNY3se3qg+LHABRJ6BooRC2dyunD+CX05CjEJxEmIUipMQo1CchBiF4iTEKBQnIUZRrZQff/R9GGvUcfVDq+WuBGi3sO0R+Lh6oKpwz5xshlP92/1rzuOX3rwC15ycY3tg9gRXwIxmuNKl1cf3Kord9kZcx6n3zUvY7rl6/VUYa1/sw1hauu2qhVJJdGuOLa7fOMW9jB7d+y8Ye1k8dx6vprjK5eImtrhefRX3poriJzB2/xi/B+ub7qqge/fuwzW+0m8JrvnCKwghXwoUJyFGoTgJMQrFSYhRKE5CjEJxEmIU1Uo5eIarUiof78DPQaMjP3SPHhAR8ZUJxPUarqao1fAlHPXdVSm1Gj6P3T3cjCtQxhks8gMcS/A5tjvue/X5EP/eh48+gLHdHNsD60tsOSSF26pYTPFYiKDA93GztwNjF99Yh7Gi7q5KOXjxAq6ZNXEzrnkXj9A484Yw5rWw9bE2cP9mHMdwjZ8oo+DRmi+8ghDypUBxEmIUipMQo1CchBiF4iTEKBQnIUZRrZS1BKfDC6XpVlG5bRZ9ti9u+tRo4b/lV3jd8a576nXu4zXada0SbB10m/hWnh7jCg1QlCJJhitxPjv8BMaOS1zxUTvAVpBfc8/yWM7wnJpVgu2G9Q387sSxMmG77f5epNsncM3jc3dzMhGR4bNdGLv3BDd6S3I8fXs4dFcgeWCSuohI5ON3B8EvJyFGoTgJMQrFSYhRKE5CjEJxEmIUipMQo6hWSn2hWCkVthy8AGjew/aAVDh1LStcGSFK46Q6uLwqxGvKAFsH8xTP1ljzceXMWg1fW7fuPseFjxtalQs8wrxb4eoeL8HnUYBmXVGCDbBmgOey5Hv4mRWijKTvukfSDzw8O6Yd4eZw6Rw/6zjEFSvNQLlu320F9UFjOxGRKqeVQsgvDBQnIUahOAkxCsVJiFEoTkKMomZrVwXOyEqptJfP3FlBT0nIlhX+P1Ep/YX8AP+o77t7ulSitcbHv9fwce+bVYY3PRcT3KumjN1ZvNkp3ui99DZgrNO/CmN5hjPRJZg23VQy7J6nvD4ezihLie+VzMF7UOHfiz2cJdWeZ7jCoybqyvTt9aZ7HMOwxM9sq4dHkSD45STEKBQnIUahOAkxCsVJiFEoTkKMQnESYhTVSkkLnHqXAqeoA6B5X7EwKsGp8kKxbZSt9FLk7s3LlWYDKZ2OKqUPTJ7i1PvkDG8C79Tdm6/rYHO1iEgjcG8OFxGROd7MLZkyDgMUHnja/29feWYBtuEqpcihBDZc4OMnnSvvojTwiISbV27C2MkxtkUCYOtkCX4Hti/gIgEEv5yEGIXiJMQoFCchRqE4CTEKxUmIUShOQoziVZVSIUAI+bnBLychRqE4CTEKxUmIUShOQoxCcRJiFIqTEKP8NyKqjJTHWMfTAAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        " \n",
        "plt.figure(figsize=(4, 4))\n",
        "image = x_train[np.random.choice(range(x_train.shape[0]))]\n",
        "plt.imshow(image.astype(\"uint8\"))\n",
        "plt.axis(\"off\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "seUfTkH9qYuQ"
      },
      "outputs": [],
      "source": [
        "def mlp(x, hidden_units, dropout_rate):\n",
        "    for units in hidden_units:\n",
        "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
        "        x = layers.Dropout(dropout_rate)(x)\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "cILjD7YpqfH1"
      },
      "outputs": [],
      "source": [
        "class Patches(layers.Layer):\n",
        "    def __init__(self, patch_size):\n",
        "        super(Patches, self).__init__()\n",
        "        self.patch_size = patch_size\n",
        " \n",
        "    def call(self, images):\n",
        "        batch_size = tf.shape(images)[0]\n",
        "        patches = tf.image.extract_patches(\n",
        "            images=images,\n",
        "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
        "            strides=[1, self.patch_size, self.patch_size, 1],\n",
        "            rates=[1, 1, 1, 1],\n",
        "            padding=\"VALID\",\n",
        "        )\n",
        "        patch_dims = patches.shape[-1]\n",
        "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
        "        return patches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6SP7q3Owqikc"
      },
      "outputs": [],
      "source": [
        "# patches = Patches(patch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "gf7Nmy7HrOnR"
      },
      "outputs": [],
      "source": [
        "class PatchEncoder(layers.Layer):\n",
        "    def __init__(self, num_patches, projection_dim):\n",
        "        super(PatchEncoder, self).__init__()\n",
        "        self.num_patches = num_patches\n",
        "        self.projection = layers.Dense(units=projection_dim)\n",
        "        self.position_embedding = layers.Embedding(\n",
        "            input_dim=num_patches, output_dim=projection_dim\n",
        "        )\n",
        " \n",
        "    def call(self, patch):\n",
        "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
        "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
        "        return encoded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "X3lr7c3mrUUr"
      },
      "outputs": [],
      "source": [
        "def create_vit_classifier():\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "    augmented = data_augmentation(inputs)\n",
        "    patches = Patches(patch_size)(augmented)\n",
        "    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
        " \n",
        "    for _ in range(transformer_layers):\n",
        "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "        attention_output = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
        "        )(x1, x1)\n",
        "        x2 = layers.Add()([attention_output, encoded_patches])\n",
        "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
        "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
        "        encoded_patches = layers.Add()([x3, x2])\n",
        " \n",
        "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "    representation = layers.Flatten()(representation)\n",
        "    representation = layers.Dropout(0.5)(representation)\n",
        "    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)\n",
        "    logits = layers.Dense(num_classes)(features)\n",
        "    model = keras.Model(inputs=inputs, outputs=logits)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uUOmtK3urmUC",
        "outputId": "e9b06e1a-2648-46f8-daa9-2ca66842c2d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow_addons in /usr/local/lib/python3.7/dist-packages (0.16.1)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow_addons) (2.7.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow_addons\n",
        "import tensorflow_addons as tfa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ZhZsUw6KsV4T"
      },
      "outputs": [],
      "source": [
        "model = create_vit_classifier()\n",
        "# optimizer = tfa.optimizers.AdamW(learning_rate=learning_rate, weight_decay=weight_decay)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "NSKNFc6JsZbG"
      },
      "outputs": [],
      "source": [
        "optimizer = tfa.optimizers.AdamW(learning_rate=learning_rate, weight_decay=weight_decay)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "EkUhRN4Ksf3q"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[\n",
        "       keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
        "       keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top-5-accuracy\"), ],)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZu4gTpcsj4e",
        "outputId": "149b11b7-abb2-4b34-c136-63f5fc14786a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "176/176 [==============================] - 168s 838ms/step - loss: 2.0519 - accuracy: 0.3101 - top-5-accuracy: 0.8068 - val_loss: 1.4929 - val_accuracy: 0.4548 - val_top-5-accuracy: 0.9134\n",
            "Epoch 2/100\n",
            "176/176 [==============================] - 146s 831ms/step - loss: 1.5619 - accuracy: 0.4373 - top-5-accuracy: 0.8990 - val_loss: 1.2895 - val_accuracy: 0.5380 - val_top-5-accuracy: 0.9444\n",
            "Epoch 3/100\n",
            "176/176 [==============================] - 146s 831ms/step - loss: 1.3980 - accuracy: 0.4966 - top-5-accuracy: 0.9267 - val_loss: 1.1914 - val_accuracy: 0.5694 - val_top-5-accuracy: 0.9610\n",
            "Epoch 4/100\n",
            "176/176 [==============================] - 146s 831ms/step - loss: 1.2979 - accuracy: 0.5415 - top-5-accuracy: 0.9373 - val_loss: 1.0870 - val_accuracy: 0.6174 - val_top-5-accuracy: 0.9632\n",
            "Epoch 5/100\n",
            "176/176 [==============================] - 146s 831ms/step - loss: 1.2245 - accuracy: 0.5669 - top-5-accuracy: 0.9450 - val_loss: 1.0025 - val_accuracy: 0.6458 - val_top-5-accuracy: 0.9702\n",
            "Epoch 6/100\n",
            "176/176 [==============================] - 146s 831ms/step - loss: 1.1745 - accuracy: 0.5867 - top-5-accuracy: 0.9510 - val_loss: 0.9843 - val_accuracy: 0.6454 - val_top-5-accuracy: 0.9736\n",
            "Epoch 7/100\n",
            "176/176 [==============================] - 146s 830ms/step - loss: 1.1077 - accuracy: 0.6126 - top-5-accuracy: 0.9570 - val_loss: 0.9038 - val_accuracy: 0.6806 - val_top-5-accuracy: 0.9770\n",
            "Epoch 8/100\n",
            "176/176 [==============================] - 146s 831ms/step - loss: 1.0742 - accuracy: 0.6229 - top-5-accuracy: 0.9596 - val_loss: 0.9077 - val_accuracy: 0.6900 - val_top-5-accuracy: 0.9768\n",
            "Epoch 9/100\n",
            "176/176 [==============================] - 147s 833ms/step - loss: 1.0166 - accuracy: 0.6418 - top-5-accuracy: 0.9655 - val_loss: 0.8624 - val_accuracy: 0.7000 - val_top-5-accuracy: 0.9804\n",
            "Epoch 10/100\n",
            "176/176 [==============================] - 147s 834ms/step - loss: 0.9840 - accuracy: 0.6534 - top-5-accuracy: 0.9678 - val_loss: 0.8200 - val_accuracy: 0.7138 - val_top-5-accuracy: 0.9828\n",
            "Epoch 11/100\n",
            "176/176 [==============================] - 147s 834ms/step - loss: 0.9421 - accuracy: 0.6719 - top-5-accuracy: 0.9707 - val_loss: 0.7934 - val_accuracy: 0.7258 - val_top-5-accuracy: 0.9824\n",
            "Epoch 12/100\n",
            "176/176 [==============================] - 147s 835ms/step - loss: 0.9108 - accuracy: 0.6839 - top-5-accuracy: 0.9727 - val_loss: 0.7645 - val_accuracy: 0.7366 - val_top-5-accuracy: 0.9818\n",
            "Epoch 13/100\n",
            "176/176 [==============================] - 147s 834ms/step - loss: 0.8691 - accuracy: 0.6971 - top-5-accuracy: 0.9754 - val_loss: 0.7230 - val_accuracy: 0.7480 - val_top-5-accuracy: 0.9826\n",
            "Epoch 14/100\n",
            "176/176 [==============================] - 146s 832ms/step - loss: 0.8416 - accuracy: 0.7067 - top-5-accuracy: 0.9774 - val_loss: 0.7189 - val_accuracy: 0.7478 - val_top-5-accuracy: 0.9862\n",
            "Epoch 15/100\n",
            "176/176 [==============================] - 146s 832ms/step - loss: 0.8140 - accuracy: 0.7169 - top-5-accuracy: 0.9794 - val_loss: 0.6933 - val_accuracy: 0.7588 - val_top-5-accuracy: 0.9870\n",
            "Epoch 16/100\n",
            "176/176 [==============================] - 146s 832ms/step - loss: 0.7917 - accuracy: 0.7250 - top-5-accuracy: 0.9798 - val_loss: 0.6669 - val_accuracy: 0.7676 - val_top-5-accuracy: 0.9862\n",
            "Epoch 17/100\n",
            "176/176 [==============================] - 147s 833ms/step - loss: 0.7698 - accuracy: 0.7307 - top-5-accuracy: 0.9812 - val_loss: 0.6610 - val_accuracy: 0.7654 - val_top-5-accuracy: 0.9880\n",
            "Epoch 18/100\n",
            "176/176 [==============================] - 146s 833ms/step - loss: 0.7474 - accuracy: 0.7389 - top-5-accuracy: 0.9823 - val_loss: 0.6496 - val_accuracy: 0.7750 - val_top-5-accuracy: 0.9888\n",
            "Epoch 19/100\n",
            "176/176 [==============================] - 146s 832ms/step - loss: 0.7260 - accuracy: 0.7456 - top-5-accuracy: 0.9840 - val_loss: 0.6366 - val_accuracy: 0.7756 - val_top-5-accuracy: 0.9882\n",
            "Epoch 20/100\n",
            "176/176 [==============================] - 147s 833ms/step - loss: 0.7136 - accuracy: 0.7496 - top-5-accuracy: 0.9833 - val_loss: 0.6132 - val_accuracy: 0.7860 - val_top-5-accuracy: 0.9896\n",
            "Epoch 21/100\n",
            "176/176 [==============================] - 147s 833ms/step - loss: 0.6941 - accuracy: 0.7582 - top-5-accuracy: 0.9845 - val_loss: 0.6397 - val_accuracy: 0.7812 - val_top-5-accuracy: 0.9898\n",
            "Epoch 22/100\n",
            "176/176 [==============================] - 146s 832ms/step - loss: 0.6745 - accuracy: 0.7652 - top-5-accuracy: 0.9858 - val_loss: 0.6048 - val_accuracy: 0.7908 - val_top-5-accuracy: 0.9890\n",
            "Epoch 23/100\n",
            "176/176 [==============================] - 146s 832ms/step - loss: 0.6621 - accuracy: 0.7685 - top-5-accuracy: 0.9861 - val_loss: 0.5911 - val_accuracy: 0.7888 - val_top-5-accuracy: 0.9916\n",
            "Epoch 24/100\n",
            "176/176 [==============================] - 146s 832ms/step - loss: 0.6359 - accuracy: 0.7765 - top-5-accuracy: 0.9877 - val_loss: 0.5921 - val_accuracy: 0.7902 - val_top-5-accuracy: 0.9910\n",
            "Epoch 25/100\n",
            "176/176 [==============================] - 146s 832ms/step - loss: 0.6357 - accuracy: 0.7761 - top-5-accuracy: 0.9884 - val_loss: 0.5852 - val_accuracy: 0.7898 - val_top-5-accuracy: 0.9914\n",
            "Epoch 26/100\n",
            "176/176 [==============================] - 146s 832ms/step - loss: 0.6111 - accuracy: 0.7832 - top-5-accuracy: 0.9891 - val_loss: 0.6285 - val_accuracy: 0.7782 - val_top-5-accuracy: 0.9920\n",
            "Epoch 27/100\n",
            "176/176 [==============================] - 146s 831ms/step - loss: 0.5928 - accuracy: 0.7921 - top-5-accuracy: 0.9893 - val_loss: 0.5790 - val_accuracy: 0.7972 - val_top-5-accuracy: 0.9908\n",
            "Epoch 28/100\n",
            "176/176 [==============================] - 146s 832ms/step - loss: 0.5873 - accuracy: 0.7930 - top-5-accuracy: 0.9902 - val_loss: 0.5634 - val_accuracy: 0.8048 - val_top-5-accuracy: 0.9918\n",
            "Epoch 29/100\n",
            "176/176 [==============================] - 146s 833ms/step - loss: 0.5722 - accuracy: 0.7990 - top-5-accuracy: 0.9895 - val_loss: 0.5575 - val_accuracy: 0.8090 - val_top-5-accuracy: 0.9922\n",
            "Epoch 30/100\n",
            "176/176 [==============================] - 146s 832ms/step - loss: 0.5584 - accuracy: 0.8042 - top-5-accuracy: 0.9909 - val_loss: 0.5660 - val_accuracy: 0.8046 - val_top-5-accuracy: 0.9904\n",
            "Epoch 31/100\n",
            "176/176 [==============================] - 146s 832ms/step - loss: 0.5471 - accuracy: 0.8069 - top-5-accuracy: 0.9909 - val_loss: 0.5472 - val_accuracy: 0.8068 - val_top-5-accuracy: 0.9928\n",
            "Epoch 32/100\n",
            "176/176 [==============================] - 147s 833ms/step - loss: 0.5341 - accuracy: 0.8124 - top-5-accuracy: 0.9919 - val_loss: 0.5555 - val_accuracy: 0.8084 - val_top-5-accuracy: 0.9912\n",
            "Epoch 33/100\n",
            "176/176 [==============================] - 146s 833ms/step - loss: 0.5221 - accuracy: 0.8166 - top-5-accuracy: 0.9924 - val_loss: 0.5569 - val_accuracy: 0.8082 - val_top-5-accuracy: 0.9928\n",
            "Epoch 34/100\n",
            "176/176 [==============================] - 146s 829ms/step - loss: 0.5058 - accuracy: 0.8222 - top-5-accuracy: 0.9929 - val_loss: 0.5496 - val_accuracy: 0.8110 - val_top-5-accuracy: 0.9902\n",
            "Epoch 35/100\n",
            "176/176 [==============================] - 146s 832ms/step - loss: 0.4961 - accuracy: 0.8256 - top-5-accuracy: 0.9928 - val_loss: 0.5465 - val_accuracy: 0.8116 - val_top-5-accuracy: 0.9914\n",
            "Epoch 36/100\n",
            "176/176 [==============================] - 146s 828ms/step - loss: 0.4945 - accuracy: 0.8282 - top-5-accuracy: 0.9930 - val_loss: 0.5212 - val_accuracy: 0.8176 - val_top-5-accuracy: 0.9914\n",
            "Epoch 37/100\n",
            "176/176 [==============================] - 145s 824ms/step - loss: 0.4846 - accuracy: 0.8264 - top-5-accuracy: 0.9937 - val_loss: 0.5514 - val_accuracy: 0.8080 - val_top-5-accuracy: 0.9898\n",
            "Epoch 38/100\n",
            "176/176 [==============================] - 145s 824ms/step - loss: 0.4690 - accuracy: 0.8332 - top-5-accuracy: 0.9939 - val_loss: 0.5279 - val_accuracy: 0.8158 - val_top-5-accuracy: 0.9934\n",
            "Epoch 39/100\n",
            "176/176 [==============================] - 145s 823ms/step - loss: 0.4628 - accuracy: 0.8371 - top-5-accuracy: 0.9942 - val_loss: 0.5050 - val_accuracy: 0.8262 - val_top-5-accuracy: 0.9934\n",
            "Epoch 40/100\n",
            "176/176 [==============================] - 145s 823ms/step - loss: 0.4435 - accuracy: 0.8444 - top-5-accuracy: 0.9948 - val_loss: 0.5154 - val_accuracy: 0.8236 - val_top-5-accuracy: 0.9906\n",
            "Epoch 41/100\n",
            "176/176 [==============================] - 145s 824ms/step - loss: 0.4376 - accuracy: 0.8470 - top-5-accuracy: 0.9956 - val_loss: 0.5265 - val_accuracy: 0.8202 - val_top-5-accuracy: 0.9916\n",
            "Epoch 42/100\n",
            "176/176 [==============================] - 145s 824ms/step - loss: 0.4275 - accuracy: 0.8481 - top-5-accuracy: 0.9957 - val_loss: 0.5314 - val_accuracy: 0.8190 - val_top-5-accuracy: 0.9918\n",
            "Epoch 43/100\n",
            "176/176 [==============================] - 145s 823ms/step - loss: 0.4187 - accuracy: 0.8510 - top-5-accuracy: 0.9959 - val_loss: 0.5350 - val_accuracy: 0.8206 - val_top-5-accuracy: 0.9904\n",
            "Epoch 44/100\n",
            "176/176 [==============================] - 145s 823ms/step - loss: 0.4118 - accuracy: 0.8548 - top-5-accuracy: 0.9954 - val_loss: 0.5001 - val_accuracy: 0.8322 - val_top-5-accuracy: 0.9926\n",
            "Epoch 45/100\n",
            "176/176 [==============================] - 145s 824ms/step - loss: 0.4083 - accuracy: 0.8561 - top-5-accuracy: 0.9963 - val_loss: 0.5263 - val_accuracy: 0.8254 - val_top-5-accuracy: 0.9914\n",
            "Epoch 46/100\n",
            "176/176 [==============================] - 145s 823ms/step - loss: 0.3932 - accuracy: 0.8623 - top-5-accuracy: 0.9962 - val_loss: 0.5213 - val_accuracy: 0.8304 - val_top-5-accuracy: 0.9906\n",
            "Epoch 47/100\n",
            "176/176 [==============================] - 145s 822ms/step - loss: 0.3918 - accuracy: 0.8640 - top-5-accuracy: 0.9963 - val_loss: 0.5095 - val_accuracy: 0.8290 - val_top-5-accuracy: 0.9924\n",
            "Epoch 48/100\n",
            "176/176 [==============================] - 145s 823ms/step - loss: 0.3883 - accuracy: 0.8645 - top-5-accuracy: 0.9960 - val_loss: 0.4946 - val_accuracy: 0.8366 - val_top-5-accuracy: 0.9938\n",
            "Epoch 49/100\n",
            "176/176 [==============================] - 145s 822ms/step - loss: 0.3774 - accuracy: 0.8664 - top-5-accuracy: 0.9967 - val_loss: 0.5055 - val_accuracy: 0.8344 - val_top-5-accuracy: 0.9924\n",
            "Epoch 50/100\n",
            "176/176 [==============================] - 145s 821ms/step - loss: 0.3717 - accuracy: 0.8688 - top-5-accuracy: 0.9967 - val_loss: 0.5149 - val_accuracy: 0.8298 - val_top-5-accuracy: 0.9914\n",
            "Epoch 51/100\n",
            "176/176 [==============================] - 145s 822ms/step - loss: 0.3708 - accuracy: 0.8690 - top-5-accuracy: 0.9964 - val_loss: 0.5184 - val_accuracy: 0.8266 - val_top-5-accuracy: 0.9922\n",
            "Epoch 52/100\n",
            "176/176 [==============================] - 146s 829ms/step - loss: 0.3606 - accuracy: 0.8749 - top-5-accuracy: 0.9967 - val_loss: 0.5219 - val_accuracy: 0.8302 - val_top-5-accuracy: 0.9912\n",
            "Epoch 53/100\n",
            "176/176 [==============================] - 146s 830ms/step - loss: 0.3438 - accuracy: 0.8790 - top-5-accuracy: 0.9977 - val_loss: 0.5220 - val_accuracy: 0.8320 - val_top-5-accuracy: 0.9922\n",
            "Epoch 54/100\n",
            "176/176 [==============================] - 146s 829ms/step - loss: 0.3582 - accuracy: 0.8730 - top-5-accuracy: 0.9972 - val_loss: 0.5370 - val_accuracy: 0.8228 - val_top-5-accuracy: 0.9908\n",
            "Epoch 55/100\n",
            "176/176 [==============================] - 146s 829ms/step - loss: 0.3472 - accuracy: 0.8787 - top-5-accuracy: 0.9974 - val_loss: 0.5287 - val_accuracy: 0.8286 - val_top-5-accuracy: 0.9918\n",
            "Epoch 56/100\n",
            "176/176 [==============================] - 146s 830ms/step - loss: 0.3440 - accuracy: 0.8798 - top-5-accuracy: 0.9973 - val_loss: 0.4922 - val_accuracy: 0.8366 - val_top-5-accuracy: 0.9926\n",
            "Epoch 57/100\n",
            "176/176 [==============================] - 146s 832ms/step - loss: 0.3342 - accuracy: 0.8825 - top-5-accuracy: 0.9975 - val_loss: 0.5107 - val_accuracy: 0.8382 - val_top-5-accuracy: 0.9914\n",
            "Epoch 58/100\n",
            "176/176 [==============================] - 146s 832ms/step - loss: 0.3262 - accuracy: 0.8858 - top-5-accuracy: 0.9975 - val_loss: 0.5037 - val_accuracy: 0.8374 - val_top-5-accuracy: 0.9920\n",
            "Epoch 59/100\n",
            "176/176 [==============================] - 146s 832ms/step - loss: 0.3261 - accuracy: 0.8862 - top-5-accuracy: 0.9978 - val_loss: 0.5040 - val_accuracy: 0.8384 - val_top-5-accuracy: 0.9922\n",
            "Epoch 60/100\n",
            "176/176 [==============================] - 147s 834ms/step - loss: 0.3210 - accuracy: 0.8869 - top-5-accuracy: 0.9980 - val_loss: 0.5413 - val_accuracy: 0.8294 - val_top-5-accuracy: 0.9926\n",
            "Epoch 61/100\n",
            "176/176 [==============================] - 146s 833ms/step - loss: 0.3220 - accuracy: 0.8880 - top-5-accuracy: 0.9981 - val_loss: 0.5337 - val_accuracy: 0.8294 - val_top-5-accuracy: 0.9906\n",
            "Epoch 62/100\n",
            "176/176 [==============================] - 146s 832ms/step - loss: 0.3135 - accuracy: 0.8921 - top-5-accuracy: 0.9982 - val_loss: 0.5373 - val_accuracy: 0.8266 - val_top-5-accuracy: 0.9920\n",
            "Epoch 63/100\n",
            "176/176 [==============================] - 146s 832ms/step - loss: 0.3144 - accuracy: 0.8911 - top-5-accuracy: 0.9978 - val_loss: 0.5330 - val_accuracy: 0.8356 - val_top-5-accuracy: 0.9924\n",
            "Epoch 64/100\n",
            "176/176 [==============================] - 147s 833ms/step - loss: 0.3132 - accuracy: 0.8911 - top-5-accuracy: 0.9982 - val_loss: 0.5081 - val_accuracy: 0.8370 - val_top-5-accuracy: 0.9922\n",
            "Epoch 65/100\n",
            "176/176 [==============================] - 146s 832ms/step - loss: 0.3016 - accuracy: 0.8932 - top-5-accuracy: 0.9981 - val_loss: 0.5186 - val_accuracy: 0.8366 - val_top-5-accuracy: 0.9916\n",
            "Epoch 66/100\n",
            "176/176 [==============================] - 146s 832ms/step - loss: 0.3053 - accuracy: 0.8922 - top-5-accuracy: 0.9983 - val_loss: 0.5352 - val_accuracy: 0.8230 - val_top-5-accuracy: 0.9928\n",
            "Epoch 67/100\n",
            "176/176 [==============================] - 146s 832ms/step - loss: 0.2960 - accuracy: 0.8961 - top-5-accuracy: 0.9980 - val_loss: 0.5005 - val_accuracy: 0.8356 - val_top-5-accuracy: 0.9922\n",
            "Epoch 68/100\n",
            "176/176 [==============================] - 146s 832ms/step - loss: 0.2925 - accuracy: 0.8980 - top-5-accuracy: 0.9981 - val_loss: 0.5058 - val_accuracy: 0.8396 - val_top-5-accuracy: 0.9938\n",
            "Epoch 69/100\n",
            "176/176 [==============================] - 146s 832ms/step - loss: 0.2864 - accuracy: 0.9016 - top-5-accuracy: 0.9984 - val_loss: 0.5428 - val_accuracy: 0.8280 - val_top-5-accuracy: 0.9924\n",
            "Epoch 70/100\n",
            "176/176 [==============================] - 146s 832ms/step - loss: 0.2848 - accuracy: 0.9011 - top-5-accuracy: 0.9982 - val_loss: 0.5437 - val_accuracy: 0.8352 - val_top-5-accuracy: 0.9922\n",
            "Epoch 71/100\n",
            "176/176 [==============================] - 146s 832ms/step - loss: 0.2815 - accuracy: 0.9020 - top-5-accuracy: 0.9985 - val_loss: 0.5068 - val_accuracy: 0.8380 - val_top-5-accuracy: 0.9926\n",
            "Epoch 72/100\n",
            "176/176 [==============================] - 147s 834ms/step - loss: 0.2844 - accuracy: 0.8992 - top-5-accuracy: 0.9986 - val_loss: 0.5345 - val_accuracy: 0.8396 - val_top-5-accuracy: 0.9934\n",
            "Epoch 73/100\n",
            "176/176 [==============================] - 147s 834ms/step - loss: 0.2860 - accuracy: 0.9000 - top-5-accuracy: 0.9984 - val_loss: 0.5221 - val_accuracy: 0.8352 - val_top-5-accuracy: 0.9912\n",
            "Epoch 74/100\n",
            "176/176 [==============================] - 147s 834ms/step - loss: 0.2741 - accuracy: 0.9051 - top-5-accuracy: 0.9984 - val_loss: 0.5155 - val_accuracy: 0.8290 - val_top-5-accuracy: 0.9932\n",
            "Epoch 75/100\n",
            "176/176 [==============================] - 147s 834ms/step - loss: 0.2783 - accuracy: 0.9036 - top-5-accuracy: 0.9984 - val_loss: 0.5316 - val_accuracy: 0.8378 - val_top-5-accuracy: 0.9926\n",
            "Epoch 76/100\n",
            "176/176 [==============================] - 147s 835ms/step - loss: 0.2725 - accuracy: 0.9057 - top-5-accuracy: 0.9983 - val_loss: 0.5115 - val_accuracy: 0.8394 - val_top-5-accuracy: 0.9932\n",
            "Epoch 77/100\n",
            "176/176 [==============================] - 147s 833ms/step - loss: 0.2698 - accuracy: 0.9056 - top-5-accuracy: 0.9985 - val_loss: 0.5333 - val_accuracy: 0.8382 - val_top-5-accuracy: 0.9914\n",
            "Epoch 78/100\n",
            "176/176 [==============================] - 147s 834ms/step - loss: 0.2672 - accuracy: 0.9058 - top-5-accuracy: 0.9988 - val_loss: 0.5312 - val_accuracy: 0.8358 - val_top-5-accuracy: 0.9928\n",
            "Epoch 79/100\n",
            "176/176 [==============================] - 147s 834ms/step - loss: 0.2664 - accuracy: 0.9070 - top-5-accuracy: 0.9986 - val_loss: 0.5073 - val_accuracy: 0.8434 - val_top-5-accuracy: 0.9926\n",
            "Epoch 80/100\n",
            "176/176 [==============================] - 147s 834ms/step - loss: 0.2618 - accuracy: 0.9094 - top-5-accuracy: 0.9989 - val_loss: 0.5085 - val_accuracy: 0.8386 - val_top-5-accuracy: 0.9912\n",
            "Epoch 81/100\n",
            "176/176 [==============================] - 147s 833ms/step - loss: 0.2590 - accuracy: 0.9093 - top-5-accuracy: 0.9985 - val_loss: 0.5583 - val_accuracy: 0.8324 - val_top-5-accuracy: 0.9926\n",
            "Epoch 82/100\n",
            "176/176 [==============================] - 146s 833ms/step - loss: 0.2628 - accuracy: 0.9089 - top-5-accuracy: 0.9983 - val_loss: 0.5462 - val_accuracy: 0.8374 - val_top-5-accuracy: 0.9938\n",
            "Epoch 83/100\n",
            "176/176 [==============================] - 147s 834ms/step - loss: 0.2558 - accuracy: 0.9121 - top-5-accuracy: 0.9986 - val_loss: 0.5113 - val_accuracy: 0.8440 - val_top-5-accuracy: 0.9928\n",
            "Epoch 84/100\n",
            "176/176 [==============================] - 147s 834ms/step - loss: 0.2509 - accuracy: 0.9127 - top-5-accuracy: 0.9984 - val_loss: 0.5582 - val_accuracy: 0.8344 - val_top-5-accuracy: 0.9928\n",
            "Epoch 85/100\n",
            "176/176 [==============================] - 147s 834ms/step - loss: 0.2559 - accuracy: 0.9114 - top-5-accuracy: 0.9987 - val_loss: 0.4769 - val_accuracy: 0.8484 - val_top-5-accuracy: 0.9926\n",
            "Epoch 86/100\n",
            "176/176 [==============================] - 147s 833ms/step - loss: 0.2539 - accuracy: 0.9121 - top-5-accuracy: 0.9985 - val_loss: 0.5397 - val_accuracy: 0.8320 - val_top-5-accuracy: 0.9926\n",
            "Epoch 87/100\n",
            "176/176 [==============================] - 147s 834ms/step - loss: 0.2499 - accuracy: 0.9129 - top-5-accuracy: 0.9988 - val_loss: 0.5132 - val_accuracy: 0.8436 - val_top-5-accuracy: 0.9942\n",
            "Epoch 88/100\n",
            "176/176 [==============================] - 147s 835ms/step - loss: 0.2446 - accuracy: 0.9148 - top-5-accuracy: 0.9988 - val_loss: 0.5248 - val_accuracy: 0.8394 - val_top-5-accuracy: 0.9928\n",
            "Epoch 89/100\n",
            "176/176 [==============================] - 147s 834ms/step - loss: 0.2483 - accuracy: 0.9152 - top-5-accuracy: 0.9987 - val_loss: 0.5230 - val_accuracy: 0.8464 - val_top-5-accuracy: 0.9922\n",
            "Epoch 90/100\n",
            "176/176 [==============================] - 147s 833ms/step - loss: 0.2422 - accuracy: 0.9156 - top-5-accuracy: 0.9989 - val_loss: 0.5068 - val_accuracy: 0.8400 - val_top-5-accuracy: 0.9914\n",
            "Epoch 91/100\n",
            "176/176 [==============================] - 147s 833ms/step - loss: 0.2473 - accuracy: 0.9144 - top-5-accuracy: 0.9988 - val_loss: 0.5226 - val_accuracy: 0.8416 - val_top-5-accuracy: 0.9932\n",
            "Epoch 92/100\n",
            "176/176 [==============================] - 147s 835ms/step - loss: 0.2434 - accuracy: 0.9159 - top-5-accuracy: 0.9987 - val_loss: 0.5519 - val_accuracy: 0.8352 - val_top-5-accuracy: 0.9918\n",
            "Epoch 93/100\n",
            "176/176 [==============================] - 147s 834ms/step - loss: 0.2442 - accuracy: 0.9171 - top-5-accuracy: 0.9990 - val_loss: 0.5250 - val_accuracy: 0.8442 - val_top-5-accuracy: 0.9932\n",
            "Epoch 94/100\n",
            "176/176 [==============================] - 147s 834ms/step - loss: 0.2372 - accuracy: 0.9172 - top-5-accuracy: 0.9986 - val_loss: 0.5022 - val_accuracy: 0.8452 - val_top-5-accuracy: 0.9900\n",
            "Epoch 95/100\n",
            "176/176 [==============================] - 147s 833ms/step - loss: 0.2330 - accuracy: 0.9193 - top-5-accuracy: 0.9990 - val_loss: 0.5538 - val_accuracy: 0.8356 - val_top-5-accuracy: 0.9914\n",
            "Epoch 96/100\n",
            "176/176 [==============================] - 146s 830ms/step - loss: 0.2365 - accuracy: 0.9186 - top-5-accuracy: 0.9987 - val_loss: 0.5172 - val_accuracy: 0.8492 - val_top-5-accuracy: 0.9926\n",
            "Epoch 97/100\n",
            "176/176 [==============================] - 145s 824ms/step - loss: 0.2419 - accuracy: 0.9168 - top-5-accuracy: 0.9987 - val_loss: 0.5018 - val_accuracy: 0.8442 - val_top-5-accuracy: 0.9928\n",
            "Epoch 98/100\n",
            "176/176 [==============================] - 146s 832ms/step - loss: 0.2291 - accuracy: 0.9212 - top-5-accuracy: 0.9986 - val_loss: 0.5275 - val_accuracy: 0.8438 - val_top-5-accuracy: 0.9936\n",
            "Epoch 99/100\n",
            "176/176 [==============================] - 147s 835ms/step - loss: 0.2318 - accuracy: 0.9200 - top-5-accuracy: 0.9989 - val_loss: 0.5273 - val_accuracy: 0.8396 - val_top-5-accuracy: 0.9920\n",
            "Epoch 100/100\n",
            "176/176 [==============================] - 147s 835ms/step - loss: 0.2293 - accuracy: 0.9213 - top-5-accuracy: 0.9989 - val_loss: 0.5334 - val_accuracy: 0.8506 - val_top-5-accuracy: 0.9932\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(\n",
        "    x=x_train,\n",
        "    y=y_train,\n",
        "    batch_size=batch_size,\n",
        "    epochs=num_epochs,\n",
        "    validation_split=0.1,)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyyaml h5py "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zmERgOhmSvfr",
        "outputId": "de7f059d-2166-491a-81c2-9439b8ef240a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (3.13)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (3.1.0)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from h5py) (1.21.6)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py) (1.5.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# assign location\n",
        "path='./cifar_transformer/Weights_folder/Weights'"
      ],
      "metadata": {
        "id": "89FuOl0cTdL0"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gBTU1rIST9Gn",
        "outputId": "99dc5a35-81f3-40ce-a604-820b1915aa88"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, query_layer_call_fn while saving (showing 5 of 100). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: ./cifar_transformer/Weights_folder/Weights/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: ./cifar_transformer/Weights_folder/Weights/assets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_weights('./cifar_transformer/h5/my_model.h5') "
      ],
      "metadata": {
        "id": "QboNgOIBV8bb"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pkPeROgMZ6y2",
        "outputId": "9199de36-1471-4db4-e4e5-95d51624503d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 32, 32, 3)]  0           []                               \n",
            "                                                                                                  \n",
            " data_augmentation (Sequential)  (None, 72, 72, 3)   7           ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " patches (Patches)              (None, None, 108)    0           ['data_augmentation[0][0]']      \n",
            "                                                                                                  \n",
            " patch_encoder (PatchEncoder)   (None, 144, 64)      16192       ['patches[0][0]']                \n",
            "                                                                                                  \n",
            " layer_normalization (LayerNorm  (None, 144, 64)     128         ['patch_encoder[0][0]']          \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " multi_head_attention (MultiHea  (None, 144, 64)     66368       ['layer_normalization[0][0]',    \n",
            " dAttention)                                                      'layer_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " add (Add)                      (None, 144, 64)      0           ['multi_head_attention[0][0]',   \n",
            "                                                                  'patch_encoder[0][0]']          \n",
            "                                                                                                  \n",
            " layer_normalization_1 (LayerNo  (None, 144, 64)     128         ['add[0][0]']                    \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 144, 128)     8320        ['layer_normalization_1[0][0]']  \n",
            "                                                                                                  \n",
            " dropout (Dropout)              (None, 144, 128)     0           ['dense_1[0][0]']                \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 144, 64)      8256        ['dropout[0][0]']                \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)            (None, 144, 64)      0           ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " add_1 (Add)                    (None, 144, 64)      0           ['dropout_1[0][0]',              \n",
            "                                                                  'add[0][0]']                    \n",
            "                                                                                                  \n",
            " layer_normalization_2 (LayerNo  (None, 144, 64)     128         ['add_1[0][0]']                  \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " multi_head_attention_1 (MultiH  (None, 144, 64)     66368       ['layer_normalization_2[0][0]',  \n",
            " eadAttention)                                                    'layer_normalization_2[0][0]']  \n",
            "                                                                                                  \n",
            " add_2 (Add)                    (None, 144, 64)      0           ['multi_head_attention_1[0][0]', \n",
            "                                                                  'add_1[0][0]']                  \n",
            "                                                                                                  \n",
            " layer_normalization_3 (LayerNo  (None, 144, 64)     128         ['add_2[0][0]']                  \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 144, 128)     8320        ['layer_normalization_3[0][0]']  \n",
            "                                                                                                  \n",
            " dropout_2 (Dropout)            (None, 144, 128)     0           ['dense_3[0][0]']                \n",
            "                                                                                                  \n",
            " dense_4 (Dense)                (None, 144, 64)      8256        ['dropout_2[0][0]']              \n",
            "                                                                                                  \n",
            " dropout_3 (Dropout)            (None, 144, 64)      0           ['dense_4[0][0]']                \n",
            "                                                                                                  \n",
            " add_3 (Add)                    (None, 144, 64)      0           ['dropout_3[0][0]',              \n",
            "                                                                  'add_2[0][0]']                  \n",
            "                                                                                                  \n",
            " layer_normalization_4 (LayerNo  (None, 144, 64)     128         ['add_3[0][0]']                  \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " multi_head_attention_2 (MultiH  (None, 144, 64)     66368       ['layer_normalization_4[0][0]',  \n",
            " eadAttention)                                                    'layer_normalization_4[0][0]']  \n",
            "                                                                                                  \n",
            " add_4 (Add)                    (None, 144, 64)      0           ['multi_head_attention_2[0][0]', \n",
            "                                                                  'add_3[0][0]']                  \n",
            "                                                                                                  \n",
            " layer_normalization_5 (LayerNo  (None, 144, 64)     128         ['add_4[0][0]']                  \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " dense_5 (Dense)                (None, 144, 128)     8320        ['layer_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " dropout_4 (Dropout)            (None, 144, 128)     0           ['dense_5[0][0]']                \n",
            "                                                                                                  \n",
            " dense_6 (Dense)                (None, 144, 64)      8256        ['dropout_4[0][0]']              \n",
            "                                                                                                  \n",
            " dropout_5 (Dropout)            (None, 144, 64)      0           ['dense_6[0][0]']                \n",
            "                                                                                                  \n",
            " add_5 (Add)                    (None, 144, 64)      0           ['dropout_5[0][0]',              \n",
            "                                                                  'add_4[0][0]']                  \n",
            "                                                                                                  \n",
            " layer_normalization_6 (LayerNo  (None, 144, 64)     128         ['add_5[0][0]']                  \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " multi_head_attention_3 (MultiH  (None, 144, 64)     66368       ['layer_normalization_6[0][0]',  \n",
            " eadAttention)                                                    'layer_normalization_6[0][0]']  \n",
            "                                                                                                  \n",
            " add_6 (Add)                    (None, 144, 64)      0           ['multi_head_attention_3[0][0]', \n",
            "                                                                  'add_5[0][0]']                  \n",
            "                                                                                                  \n",
            " layer_normalization_7 (LayerNo  (None, 144, 64)     128         ['add_6[0][0]']                  \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " dense_7 (Dense)                (None, 144, 128)     8320        ['layer_normalization_7[0][0]']  \n",
            "                                                                                                  \n",
            " dropout_6 (Dropout)            (None, 144, 128)     0           ['dense_7[0][0]']                \n",
            "                                                                                                  \n",
            " dense_8 (Dense)                (None, 144, 64)      8256        ['dropout_6[0][0]']              \n",
            "                                                                                                  \n",
            " dropout_7 (Dropout)            (None, 144, 64)      0           ['dense_8[0][0]']                \n",
            "                                                                                                  \n",
            " add_7 (Add)                    (None, 144, 64)      0           ['dropout_7[0][0]',              \n",
            "                                                                  'add_6[0][0]']                  \n",
            "                                                                                                  \n",
            " layer_normalization_8 (LayerNo  (None, 144, 64)     128         ['add_7[0][0]']                  \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " multi_head_attention_4 (MultiH  (None, 144, 64)     66368       ['layer_normalization_8[0][0]',  \n",
            " eadAttention)                                                    'layer_normalization_8[0][0]']  \n",
            "                                                                                                  \n",
            " add_8 (Add)                    (None, 144, 64)      0           ['multi_head_attention_4[0][0]', \n",
            "                                                                  'add_7[0][0]']                  \n",
            "                                                                                                  \n",
            " layer_normalization_9 (LayerNo  (None, 144, 64)     128         ['add_8[0][0]']                  \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " dense_9 (Dense)                (None, 144, 128)     8320        ['layer_normalization_9[0][0]']  \n",
            "                                                                                                  \n",
            " dropout_8 (Dropout)            (None, 144, 128)     0           ['dense_9[0][0]']                \n",
            "                                                                                                  \n",
            " dense_10 (Dense)               (None, 144, 64)      8256        ['dropout_8[0][0]']              \n",
            "                                                                                                  \n",
            " dropout_9 (Dropout)            (None, 144, 64)      0           ['dense_10[0][0]']               \n",
            "                                                                                                  \n",
            " add_9 (Add)                    (None, 144, 64)      0           ['dropout_9[0][0]',              \n",
            "                                                                  'add_8[0][0]']                  \n",
            "                                                                                                  \n",
            " layer_normalization_10 (LayerN  (None, 144, 64)     128         ['add_9[0][0]']                  \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_5 (MultiH  (None, 144, 64)     66368       ['layer_normalization_10[0][0]', \n",
            " eadAttention)                                                    'layer_normalization_10[0][0]'] \n",
            "                                                                                                  \n",
            " add_10 (Add)                   (None, 144, 64)      0           ['multi_head_attention_5[0][0]', \n",
            "                                                                  'add_9[0][0]']                  \n",
            "                                                                                                  \n",
            " layer_normalization_11 (LayerN  (None, 144, 64)     128         ['add_10[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dense_11 (Dense)               (None, 144, 128)     8320        ['layer_normalization_11[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_10 (Dropout)           (None, 144, 128)     0           ['dense_11[0][0]']               \n",
            "                                                                                                  \n",
            " dense_12 (Dense)               (None, 144, 64)      8256        ['dropout_10[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_11 (Dropout)           (None, 144, 64)      0           ['dense_12[0][0]']               \n",
            "                                                                                                  \n",
            " add_11 (Add)                   (None, 144, 64)      0           ['dropout_11[0][0]',             \n",
            "                                                                  'add_10[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_12 (LayerN  (None, 144, 64)     128         ['add_11[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_6 (MultiH  (None, 144, 64)     66368       ['layer_normalization_12[0][0]', \n",
            " eadAttention)                                                    'layer_normalization_12[0][0]'] \n",
            "                                                                                                  \n",
            " add_12 (Add)                   (None, 144, 64)      0           ['multi_head_attention_6[0][0]', \n",
            "                                                                  'add_11[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_13 (LayerN  (None, 144, 64)     128         ['add_12[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dense_13 (Dense)               (None, 144, 128)     8320        ['layer_normalization_13[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_12 (Dropout)           (None, 144, 128)     0           ['dense_13[0][0]']               \n",
            "                                                                                                  \n",
            " dense_14 (Dense)               (None, 144, 64)      8256        ['dropout_12[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_13 (Dropout)           (None, 144, 64)      0           ['dense_14[0][0]']               \n",
            "                                                                                                  \n",
            " add_13 (Add)                   (None, 144, 64)      0           ['dropout_13[0][0]',             \n",
            "                                                                  'add_12[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_14 (LayerN  (None, 144, 64)     128         ['add_13[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_7 (MultiH  (None, 144, 64)     66368       ['layer_normalization_14[0][0]', \n",
            " eadAttention)                                                    'layer_normalization_14[0][0]'] \n",
            "                                                                                                  \n",
            " add_14 (Add)                   (None, 144, 64)      0           ['multi_head_attention_7[0][0]', \n",
            "                                                                  'add_13[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_15 (LayerN  (None, 144, 64)     128         ['add_14[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dense_15 (Dense)               (None, 144, 128)     8320        ['layer_normalization_15[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_14 (Dropout)           (None, 144, 128)     0           ['dense_15[0][0]']               \n",
            "                                                                                                  \n",
            " dense_16 (Dense)               (None, 144, 64)      8256        ['dropout_14[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_15 (Dropout)           (None, 144, 64)      0           ['dense_16[0][0]']               \n",
            "                                                                                                  \n",
            " add_15 (Add)                   (None, 144, 64)      0           ['dropout_15[0][0]',             \n",
            "                                                                  'add_14[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_16 (LayerN  (None, 144, 64)     128         ['add_15[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 9216)         0           ['layer_normalization_16[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_16 (Dropout)           (None, 9216)         0           ['flatten[0][0]']                \n",
            "                                                                                                  \n",
            " dense_17 (Dense)               (None, 2048)         18876416    ['dropout_16[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_17 (Dropout)           (None, 2048)         0           ['dense_17[0][0]']               \n",
            "                                                                                                  \n",
            " dense_18 (Dense)               (None, 1024)         2098176     ['dropout_17[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_18 (Dropout)           (None, 1024)         0           ['dense_18[0][0]']               \n",
            "                                                                                                  \n",
            " dense_19 (Dense)               (None, 10)           10250       ['dropout_18[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 21,666,769\n",
            "Trainable params: 21,666,762\n",
            "Non-trainable params: 7\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "loaded_model = keras.models.load_model(path) "
      ],
      "metadata": {
        "id": "mNXJXgPCauil"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5nLMbjNAbxFk",
        "outputId": "750ffacc-f24b-4af5-bbe9-9acd34dd1cac"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 32, 32, 3)]  0           []                               \n",
            "                                                                                                  \n",
            " data_augmentation (Sequential)  (None, 72, 72, 3)   7           ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " patches (Patches)              (None, None, 108)    0           ['data_augmentation[0][0]']      \n",
            "                                                                                                  \n",
            " patch_encoder (PatchEncoder)   (None, 144, 64)      16192       ['patches[0][0]']                \n",
            "                                                                                                  \n",
            " layer_normalization (LayerNorm  (None, 144, 64)     128         ['patch_encoder[0][0]']          \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " multi_head_attention (MultiHea  (None, 144, 64)     66368       ['layer_normalization[0][0]',    \n",
            " dAttention)                                                      'layer_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " add (Add)                      (None, 144, 64)      0           ['multi_head_attention[0][0]',   \n",
            "                                                                  'patch_encoder[0][0]']          \n",
            "                                                                                                  \n",
            " layer_normalization_1 (LayerNo  (None, 144, 64)     128         ['add[0][0]']                    \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 144, 128)     8320        ['layer_normalization_1[0][0]']  \n",
            "                                                                                                  \n",
            " dropout (Dropout)              (None, 144, 128)     0           ['dense_1[0][0]']                \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 144, 64)      8256        ['dropout[0][0]']                \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)            (None, 144, 64)      0           ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " add_1 (Add)                    (None, 144, 64)      0           ['dropout_1[0][0]',              \n",
            "                                                                  'add[0][0]']                    \n",
            "                                                                                                  \n",
            " layer_normalization_2 (LayerNo  (None, 144, 64)     128         ['add_1[0][0]']                  \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " multi_head_attention_1 (MultiH  (None, 144, 64)     66368       ['layer_normalization_2[0][0]',  \n",
            " eadAttention)                                                    'layer_normalization_2[0][0]']  \n",
            "                                                                                                  \n",
            " add_2 (Add)                    (None, 144, 64)      0           ['multi_head_attention_1[0][0]', \n",
            "                                                                  'add_1[0][0]']                  \n",
            "                                                                                                  \n",
            " layer_normalization_3 (LayerNo  (None, 144, 64)     128         ['add_2[0][0]']                  \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 144, 128)     8320        ['layer_normalization_3[0][0]']  \n",
            "                                                                                                  \n",
            " dropout_2 (Dropout)            (None, 144, 128)     0           ['dense_3[0][0]']                \n",
            "                                                                                                  \n",
            " dense_4 (Dense)                (None, 144, 64)      8256        ['dropout_2[0][0]']              \n",
            "                                                                                                  \n",
            " dropout_3 (Dropout)            (None, 144, 64)      0           ['dense_4[0][0]']                \n",
            "                                                                                                  \n",
            " add_3 (Add)                    (None, 144, 64)      0           ['dropout_3[0][0]',              \n",
            "                                                                  'add_2[0][0]']                  \n",
            "                                                                                                  \n",
            " layer_normalization_4 (LayerNo  (None, 144, 64)     128         ['add_3[0][0]']                  \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " multi_head_attention_2 (MultiH  (None, 144, 64)     66368       ['layer_normalization_4[0][0]',  \n",
            " eadAttention)                                                    'layer_normalization_4[0][0]']  \n",
            "                                                                                                  \n",
            " add_4 (Add)                    (None, 144, 64)      0           ['multi_head_attention_2[0][0]', \n",
            "                                                                  'add_3[0][0]']                  \n",
            "                                                                                                  \n",
            " layer_normalization_5 (LayerNo  (None, 144, 64)     128         ['add_4[0][0]']                  \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " dense_5 (Dense)                (None, 144, 128)     8320        ['layer_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " dropout_4 (Dropout)            (None, 144, 128)     0           ['dense_5[0][0]']                \n",
            "                                                                                                  \n",
            " dense_6 (Dense)                (None, 144, 64)      8256        ['dropout_4[0][0]']              \n",
            "                                                                                                  \n",
            " dropout_5 (Dropout)            (None, 144, 64)      0           ['dense_6[0][0]']                \n",
            "                                                                                                  \n",
            " add_5 (Add)                    (None, 144, 64)      0           ['dropout_5[0][0]',              \n",
            "                                                                  'add_4[0][0]']                  \n",
            "                                                                                                  \n",
            " layer_normalization_6 (LayerNo  (None, 144, 64)     128         ['add_5[0][0]']                  \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " multi_head_attention_3 (MultiH  (None, 144, 64)     66368       ['layer_normalization_6[0][0]',  \n",
            " eadAttention)                                                    'layer_normalization_6[0][0]']  \n",
            "                                                                                                  \n",
            " add_6 (Add)                    (None, 144, 64)      0           ['multi_head_attention_3[0][0]', \n",
            "                                                                  'add_5[0][0]']                  \n",
            "                                                                                                  \n",
            " layer_normalization_7 (LayerNo  (None, 144, 64)     128         ['add_6[0][0]']                  \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " dense_7 (Dense)                (None, 144, 128)     8320        ['layer_normalization_7[0][0]']  \n",
            "                                                                                                  \n",
            " dropout_6 (Dropout)            (None, 144, 128)     0           ['dense_7[0][0]']                \n",
            "                                                                                                  \n",
            " dense_8 (Dense)                (None, 144, 64)      8256        ['dropout_6[0][0]']              \n",
            "                                                                                                  \n",
            " dropout_7 (Dropout)            (None, 144, 64)      0           ['dense_8[0][0]']                \n",
            "                                                                                                  \n",
            " add_7 (Add)                    (None, 144, 64)      0           ['dropout_7[0][0]',              \n",
            "                                                                  'add_6[0][0]']                  \n",
            "                                                                                                  \n",
            " layer_normalization_8 (LayerNo  (None, 144, 64)     128         ['add_7[0][0]']                  \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " multi_head_attention_4 (MultiH  (None, 144, 64)     66368       ['layer_normalization_8[0][0]',  \n",
            " eadAttention)                                                    'layer_normalization_8[0][0]']  \n",
            "                                                                                                  \n",
            " add_8 (Add)                    (None, 144, 64)      0           ['multi_head_attention_4[0][0]', \n",
            "                                                                  'add_7[0][0]']                  \n",
            "                                                                                                  \n",
            " layer_normalization_9 (LayerNo  (None, 144, 64)     128         ['add_8[0][0]']                  \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " dense_9 (Dense)                (None, 144, 128)     8320        ['layer_normalization_9[0][0]']  \n",
            "                                                                                                  \n",
            " dropout_8 (Dropout)            (None, 144, 128)     0           ['dense_9[0][0]']                \n",
            "                                                                                                  \n",
            " dense_10 (Dense)               (None, 144, 64)      8256        ['dropout_8[0][0]']              \n",
            "                                                                                                  \n",
            " dropout_9 (Dropout)            (None, 144, 64)      0           ['dense_10[0][0]']               \n",
            "                                                                                                  \n",
            " add_9 (Add)                    (None, 144, 64)      0           ['dropout_9[0][0]',              \n",
            "                                                                  'add_8[0][0]']                  \n",
            "                                                                                                  \n",
            " layer_normalization_10 (LayerN  (None, 144, 64)     128         ['add_9[0][0]']                  \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_5 (MultiH  (None, 144, 64)     66368       ['layer_normalization_10[0][0]', \n",
            " eadAttention)                                                    'layer_normalization_10[0][0]'] \n",
            "                                                                                                  \n",
            " add_10 (Add)                   (None, 144, 64)      0           ['multi_head_attention_5[0][0]', \n",
            "                                                                  'add_9[0][0]']                  \n",
            "                                                                                                  \n",
            " layer_normalization_11 (LayerN  (None, 144, 64)     128         ['add_10[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dense_11 (Dense)               (None, 144, 128)     8320        ['layer_normalization_11[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_10 (Dropout)           (None, 144, 128)     0           ['dense_11[0][0]']               \n",
            "                                                                                                  \n",
            " dense_12 (Dense)               (None, 144, 64)      8256        ['dropout_10[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_11 (Dropout)           (None, 144, 64)      0           ['dense_12[0][0]']               \n",
            "                                                                                                  \n",
            " add_11 (Add)                   (None, 144, 64)      0           ['dropout_11[0][0]',             \n",
            "                                                                  'add_10[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_12 (LayerN  (None, 144, 64)     128         ['add_11[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_6 (MultiH  (None, 144, 64)     66368       ['layer_normalization_12[0][0]', \n",
            " eadAttention)                                                    'layer_normalization_12[0][0]'] \n",
            "                                                                                                  \n",
            " add_12 (Add)                   (None, 144, 64)      0           ['multi_head_attention_6[0][0]', \n",
            "                                                                  'add_11[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_13 (LayerN  (None, 144, 64)     128         ['add_12[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dense_13 (Dense)               (None, 144, 128)     8320        ['layer_normalization_13[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_12 (Dropout)           (None, 144, 128)     0           ['dense_13[0][0]']               \n",
            "                                                                                                  \n",
            " dense_14 (Dense)               (None, 144, 64)      8256        ['dropout_12[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_13 (Dropout)           (None, 144, 64)      0           ['dense_14[0][0]']               \n",
            "                                                                                                  \n",
            " add_13 (Add)                   (None, 144, 64)      0           ['dropout_13[0][0]',             \n",
            "                                                                  'add_12[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_14 (LayerN  (None, 144, 64)     128         ['add_13[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_7 (MultiH  (None, 144, 64)     66368       ['layer_normalization_14[0][0]', \n",
            " eadAttention)                                                    'layer_normalization_14[0][0]'] \n",
            "                                                                                                  \n",
            " add_14 (Add)                   (None, 144, 64)      0           ['multi_head_attention_7[0][0]', \n",
            "                                                                  'add_13[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_15 (LayerN  (None, 144, 64)     128         ['add_14[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " dense_15 (Dense)               (None, 144, 128)     8320        ['layer_normalization_15[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_14 (Dropout)           (None, 144, 128)     0           ['dense_15[0][0]']               \n",
            "                                                                                                  \n",
            " dense_16 (Dense)               (None, 144, 64)      8256        ['dropout_14[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_15 (Dropout)           (None, 144, 64)      0           ['dense_16[0][0]']               \n",
            "                                                                                                  \n",
            " add_15 (Add)                   (None, 144, 64)      0           ['dropout_15[0][0]',             \n",
            "                                                                  'add_14[0][0]']                 \n",
            "                                                                                                  \n",
            " layer_normalization_16 (LayerN  (None, 144, 64)     128         ['add_15[0][0]']                 \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 9216)         0           ['layer_normalization_16[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_16 (Dropout)           (None, 9216)         0           ['flatten[0][0]']                \n",
            "                                                                                                  \n",
            " dense_17 (Dense)               (None, 2048)         18876416    ['dropout_16[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_17 (Dropout)           (None, 2048)         0           ['dense_17[0][0]']               \n",
            "                                                                                                  \n",
            " dense_18 (Dense)               (None, 1024)         2098176     ['dropout_17[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_18 (Dropout)           (None, 1024)         0           ['dense_18[0][0]']               \n",
            "                                                                                                  \n",
            " dense_19 (Dense)               (None, 10)           10250       ['dropout_18[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 21,666,769\n",
            "Trainable params: 21,666,762\n",
            "Non-trainable params: 7\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "cifar-transformer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMSMLjfm1dFQIYibMIo6RSX",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}